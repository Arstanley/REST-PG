{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.648504983388704,
  "eval_steps": 500,
  "global_step": 500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.026578073089700997,
      "grad_norm": 0.6958180069923401,
      "learning_rate": 2.0000000000000002e-07,
      "loss": 1.3217,
      "step": 5
    },
    {
      "epoch": 0.053156146179401995,
      "grad_norm": 0.6409521698951721,
      "learning_rate": 4.5000000000000003e-07,
      "loss": 0.8385,
      "step": 10
    },
    {
      "epoch": 0.07973421926910298,
      "grad_norm": 0.4375435709953308,
      "learning_rate": 7.000000000000001e-07,
      "loss": 0.9572,
      "step": 15
    },
    {
      "epoch": 0.10631229235880399,
      "grad_norm": 0.3956605792045593,
      "learning_rate": 9.500000000000001e-07,
      "loss": 1.4908,
      "step": 20
    },
    {
      "epoch": 0.132890365448505,
      "grad_norm": 0.36361178755760193,
      "learning_rate": 1.2000000000000002e-06,
      "loss": 1.1338,
      "step": 25
    },
    {
      "epoch": 0.15946843853820597,
      "grad_norm": 0.42190778255462646,
      "learning_rate": 1.45e-06,
      "loss": 1.1238,
      "step": 30
    },
    {
      "epoch": 0.18604651162790697,
      "grad_norm": 0.5130558013916016,
      "learning_rate": 1.7000000000000002e-06,
      "loss": 1.6133,
      "step": 35
    },
    {
      "epoch": 0.21262458471760798,
      "grad_norm": 0.6562350988388062,
      "learning_rate": 1.9500000000000004e-06,
      "loss": 1.2576,
      "step": 40
    },
    {
      "epoch": 0.23920265780730898,
      "grad_norm": 0.42076975107192993,
      "learning_rate": 2.2e-06,
      "loss": 1.3102,
      "step": 45
    },
    {
      "epoch": 0.26578073089701,
      "grad_norm": 1.337512731552124,
      "learning_rate": 2.4500000000000003e-06,
      "loss": 1.3027,
      "step": 50
    },
    {
      "epoch": 0.292358803986711,
      "grad_norm": 0.40282195806503296,
      "learning_rate": 2.7000000000000004e-06,
      "loss": 1.0708,
      "step": 55
    },
    {
      "epoch": 0.31893687707641194,
      "grad_norm": 0.6363669633865356,
      "learning_rate": 2.95e-06,
      "loss": 1.3902,
      "step": 60
    },
    {
      "epoch": 0.34551495016611294,
      "grad_norm": 0.9930053353309631,
      "learning_rate": 3.2000000000000003e-06,
      "loss": 1.5482,
      "step": 65
    },
    {
      "epoch": 0.37209302325581395,
      "grad_norm": 0.5784740447998047,
      "learning_rate": 3.45e-06,
      "loss": 1.7897,
      "step": 70
    },
    {
      "epoch": 0.39867109634551495,
      "grad_norm": 0.4972952604293823,
      "learning_rate": 3.7e-06,
      "loss": 1.284,
      "step": 75
    },
    {
      "epoch": 0.42524916943521596,
      "grad_norm": 0.29334723949432373,
      "learning_rate": 3.95e-06,
      "loss": 1.1897,
      "step": 80
    },
    {
      "epoch": 0.45182724252491696,
      "grad_norm": 0.7421553134918213,
      "learning_rate": 4.2000000000000004e-06,
      "loss": 1.5209,
      "step": 85
    },
    {
      "epoch": 0.47840531561461797,
      "grad_norm": 0.4296242594718933,
      "learning_rate": 4.450000000000001e-06,
      "loss": 1.3573,
      "step": 90
    },
    {
      "epoch": 0.5049833887043189,
      "grad_norm": 0.9992793202400208,
      "learning_rate": 4.7e-06,
      "loss": 1.4423,
      "step": 95
    },
    {
      "epoch": 0.53156146179402,
      "grad_norm": 0.5346852540969849,
      "learning_rate": 4.95e-06,
      "loss": 1.314,
      "step": 100
    },
    {
      "epoch": 0.5581395348837209,
      "grad_norm": 0.8384026885032654,
      "learning_rate": 4.957173447537474e-06,
      "loss": 1.1551,
      "step": 105
    },
    {
      "epoch": 0.584717607973422,
      "grad_norm": 0.3455433249473572,
      "learning_rate": 4.903640256959315e-06,
      "loss": 0.9861,
      "step": 110
    },
    {
      "epoch": 0.6112956810631229,
      "grad_norm": 0.6525814533233643,
      "learning_rate": 4.850107066381157e-06,
      "loss": 1.4932,
      "step": 115
    },
    {
      "epoch": 0.6378737541528239,
      "grad_norm": 0.3529279828071594,
      "learning_rate": 4.796573875802998e-06,
      "loss": 0.8233,
      "step": 120
    },
    {
      "epoch": 0.6644518272425249,
      "grad_norm": 0.44600483775138855,
      "learning_rate": 4.743040685224839e-06,
      "loss": 1.2997,
      "step": 125
    },
    {
      "epoch": 0.6910299003322259,
      "grad_norm": 0.5020657181739807,
      "learning_rate": 4.689507494646681e-06,
      "loss": 1.4756,
      "step": 130
    },
    {
      "epoch": 0.717607973421927,
      "grad_norm": 0.28874993324279785,
      "learning_rate": 4.6359743040685226e-06,
      "loss": 1.4332,
      "step": 135
    },
    {
      "epoch": 0.7441860465116279,
      "grad_norm": 0.499350368976593,
      "learning_rate": 4.582441113490364e-06,
      "loss": 1.068,
      "step": 140
    },
    {
      "epoch": 0.770764119601329,
      "grad_norm": 0.3056113123893738,
      "learning_rate": 4.528907922912206e-06,
      "loss": 1.0535,
      "step": 145
    },
    {
      "epoch": 0.7973421926910299,
      "grad_norm": 0.6307733058929443,
      "learning_rate": 4.4753747323340476e-06,
      "loss": 1.4611,
      "step": 150
    },
    {
      "epoch": 0.8239202657807309,
      "grad_norm": 0.5168638229370117,
      "learning_rate": 4.421841541755889e-06,
      "loss": 1.0234,
      "step": 155
    },
    {
      "epoch": 0.8504983388704319,
      "grad_norm": 0.3545894920825958,
      "learning_rate": 4.368308351177731e-06,
      "loss": 1.0507,
      "step": 160
    },
    {
      "epoch": 0.8770764119601329,
      "grad_norm": 1.338249683380127,
      "learning_rate": 4.314775160599572e-06,
      "loss": 0.9097,
      "step": 165
    },
    {
      "epoch": 0.9036544850498339,
      "grad_norm": 0.5473935604095459,
      "learning_rate": 4.261241970021413e-06,
      "loss": 1.0303,
      "step": 170
    },
    {
      "epoch": 0.9302325581395349,
      "grad_norm": 0.4043974280357361,
      "learning_rate": 4.207708779443255e-06,
      "loss": 0.9606,
      "step": 175
    },
    {
      "epoch": 0.9568106312292359,
      "grad_norm": 1.624180555343628,
      "learning_rate": 4.154175588865097e-06,
      "loss": 1.1817,
      "step": 180
    },
    {
      "epoch": 0.9833887043189369,
      "grad_norm": 0.3997002840042114,
      "learning_rate": 4.100642398286938e-06,
      "loss": 0.8498,
      "step": 185
    },
    {
      "epoch": 1.0053156146179403,
      "grad_norm": 0.6074920892715454,
      "learning_rate": 4.04710920770878e-06,
      "loss": 1.3983,
      "step": 190
    },
    {
      "epoch": 1.0318936877076412,
      "grad_norm": 0.5274844765663147,
      "learning_rate": 3.993576017130622e-06,
      "loss": 1.1327,
      "step": 195
    },
    {
      "epoch": 1.0584717607973422,
      "grad_norm": 1.8997836112976074,
      "learning_rate": 3.9400428265524625e-06,
      "loss": 1.0596,
      "step": 200
    },
    {
      "epoch": 1.0850498338870431,
      "grad_norm": 0.4736908972263336,
      "learning_rate": 3.886509635974304e-06,
      "loss": 1.0646,
      "step": 205
    },
    {
      "epoch": 1.1116279069767443,
      "grad_norm": 0.9171909093856812,
      "learning_rate": 3.832976445396146e-06,
      "loss": 1.0518,
      "step": 210
    },
    {
      "epoch": 1.1382059800664452,
      "grad_norm": 1.3624159097671509,
      "learning_rate": 3.7794432548179875e-06,
      "loss": 1.0571,
      "step": 215
    },
    {
      "epoch": 1.1647840531561462,
      "grad_norm": 0.959736704826355,
      "learning_rate": 3.7259100642398288e-06,
      "loss": 1.397,
      "step": 220
    },
    {
      "epoch": 1.1913621262458471,
      "grad_norm": 0.48194754123687744,
      "learning_rate": 3.6723768736616704e-06,
      "loss": 1.0207,
      "step": 225
    },
    {
      "epoch": 1.217940199335548,
      "grad_norm": 0.3648807108402252,
      "learning_rate": 3.618843683083512e-06,
      "loss": 1.1269,
      "step": 230
    },
    {
      "epoch": 1.2445182724252493,
      "grad_norm": 1.6377309560775757,
      "learning_rate": 3.5653104925053538e-06,
      "loss": 0.8463,
      "step": 235
    },
    {
      "epoch": 1.2710963455149502,
      "grad_norm": 0.7982916235923767,
      "learning_rate": 3.511777301927195e-06,
      "loss": 1.0301,
      "step": 240
    },
    {
      "epoch": 1.2976744186046512,
      "grad_norm": 0.43531641364097595,
      "learning_rate": 3.4582441113490367e-06,
      "loss": 0.7587,
      "step": 245
    },
    {
      "epoch": 1.324252491694352,
      "grad_norm": 0.44554170966148376,
      "learning_rate": 3.4047109207708783e-06,
      "loss": 0.8538,
      "step": 250
    },
    {
      "epoch": 1.350830564784053,
      "grad_norm": 0.3353292644023895,
      "learning_rate": 3.3511777301927196e-06,
      "loss": 0.8909,
      "step": 255
    },
    {
      "epoch": 1.3774086378737542,
      "grad_norm": 0.29172101616859436,
      "learning_rate": 3.2976445396145612e-06,
      "loss": 0.8318,
      "step": 260
    },
    {
      "epoch": 1.4039867109634552,
      "grad_norm": 1.1254551410675049,
      "learning_rate": 3.244111349036403e-06,
      "loss": 1.0421,
      "step": 265
    },
    {
      "epoch": 1.4305647840531561,
      "grad_norm": 0.39845895767211914,
      "learning_rate": 3.1905781584582446e-06,
      "loss": 0.9206,
      "step": 270
    },
    {
      "epoch": 1.457142857142857,
      "grad_norm": 0.5686017274856567,
      "learning_rate": 3.137044967880086e-06,
      "loss": 1.4948,
      "step": 275
    },
    {
      "epoch": 1.483720930232558,
      "grad_norm": 0.33817896246910095,
      "learning_rate": 3.0835117773019275e-06,
      "loss": 1.1423,
      "step": 280
    },
    {
      "epoch": 1.5102990033222592,
      "grad_norm": 0.41815385222435,
      "learning_rate": 3.029978586723769e-06,
      "loss": 0.888,
      "step": 285
    },
    {
      "epoch": 1.5368770764119601,
      "grad_norm": 0.6229838132858276,
      "learning_rate": 2.976445396145611e-06,
      "loss": 1.0276,
      "step": 290
    },
    {
      "epoch": 1.563455149501661,
      "grad_norm": 1.0020173788070679,
      "learning_rate": 2.922912205567452e-06,
      "loss": 1.026,
      "step": 295
    },
    {
      "epoch": 1.5900332225913623,
      "grad_norm": 0.383479505777359,
      "learning_rate": 2.8693790149892937e-06,
      "loss": 1.189,
      "step": 300
    },
    {
      "epoch": 1.616611295681063,
      "grad_norm": 0.7752323150634766,
      "learning_rate": 2.8158458244111354e-06,
      "loss": 1.0768,
      "step": 305
    },
    {
      "epoch": 1.6431893687707642,
      "grad_norm": 1.2049219608306885,
      "learning_rate": 2.7623126338329766e-06,
      "loss": 1.0459,
      "step": 310
    },
    {
      "epoch": 1.669767441860465,
      "grad_norm": 0.48924094438552856,
      "learning_rate": 2.7087794432548183e-06,
      "loss": 0.4396,
      "step": 315
    },
    {
      "epoch": 1.696345514950166,
      "grad_norm": 0.5366458892822266,
      "learning_rate": 2.65524625267666e-06,
      "loss": 0.9118,
      "step": 320
    },
    {
      "epoch": 1.7229235880398672,
      "grad_norm": 0.8387731909751892,
      "learning_rate": 2.6017130620985016e-06,
      "loss": 0.7983,
      "step": 325
    },
    {
      "epoch": 1.749501661129568,
      "grad_norm": 0.44545966386795044,
      "learning_rate": 2.5481798715203425e-06,
      "loss": 0.7733,
      "step": 330
    },
    {
      "epoch": 1.7760797342192691,
      "grad_norm": 0.4131885766983032,
      "learning_rate": 2.494646680942184e-06,
      "loss": 1.4269,
      "step": 335
    },
    {
      "epoch": 1.80265780730897,
      "grad_norm": 0.42328161001205444,
      "learning_rate": 2.441113490364026e-06,
      "loss": 1.0253,
      "step": 340
    },
    {
      "epoch": 1.829235880398671,
      "grad_norm": 0.57774418592453,
      "learning_rate": 2.3875802997858674e-06,
      "loss": 1.0858,
      "step": 345
    },
    {
      "epoch": 1.8558139534883722,
      "grad_norm": 0.2440931797027588,
      "learning_rate": 2.334047109207709e-06,
      "loss": 0.9369,
      "step": 350
    },
    {
      "epoch": 1.8823920265780731,
      "grad_norm": 0.5649392008781433,
      "learning_rate": 2.2805139186295504e-06,
      "loss": 0.9377,
      "step": 355
    },
    {
      "epoch": 1.908970099667774,
      "grad_norm": 0.29329362511634827,
      "learning_rate": 2.226980728051392e-06,
      "loss": 0.9385,
      "step": 360
    },
    {
      "epoch": 1.935548172757475,
      "grad_norm": 0.8225168585777283,
      "learning_rate": 2.1734475374732337e-06,
      "loss": 1.216,
      "step": 365
    },
    {
      "epoch": 1.962126245847176,
      "grad_norm": 0.2537192702293396,
      "learning_rate": 2.119914346895075e-06,
      "loss": 0.9068,
      "step": 370
    },
    {
      "epoch": 1.9887043189368772,
      "grad_norm": 0.2697610855102539,
      "learning_rate": 2.0663811563169166e-06,
      "loss": 1.1284,
      "step": 375
    },
    {
      "epoch": 2.0106312292358806,
      "grad_norm": 0.5250672101974487,
      "learning_rate": 2.0128479657387583e-06,
      "loss": 0.904,
      "step": 380
    },
    {
      "epoch": 2.0372093023255813,
      "grad_norm": 0.5947989225387573,
      "learning_rate": 1.9593147751606e-06,
      "loss": 1.0111,
      "step": 385
    },
    {
      "epoch": 2.0637873754152825,
      "grad_norm": 0.6636350750923157,
      "learning_rate": 1.9057815845824412e-06,
      "loss": 1.0026,
      "step": 390
    },
    {
      "epoch": 2.090365448504983,
      "grad_norm": 0.4429844617843628,
      "learning_rate": 1.8522483940042828e-06,
      "loss": 1.0777,
      "step": 395
    },
    {
      "epoch": 2.1169435215946844,
      "grad_norm": 0.27108174562454224,
      "learning_rate": 1.7987152034261243e-06,
      "loss": 1.1988,
      "step": 400
    },
    {
      "epoch": 2.1435215946843855,
      "grad_norm": 0.6250683665275574,
      "learning_rate": 1.745182012847966e-06,
      "loss": 0.7479,
      "step": 405
    },
    {
      "epoch": 2.1700996677740862,
      "grad_norm": 0.47340306639671326,
      "learning_rate": 1.6916488222698074e-06,
      "loss": 1.2871,
      "step": 410
    },
    {
      "epoch": 2.1966777408637874,
      "grad_norm": 0.8382675051689148,
      "learning_rate": 1.6381156316916489e-06,
      "loss": 1.0423,
      "step": 415
    },
    {
      "epoch": 2.2232558139534886,
      "grad_norm": 0.5472170114517212,
      "learning_rate": 1.5845824411134905e-06,
      "loss": 1.1309,
      "step": 420
    },
    {
      "epoch": 2.2498338870431893,
      "grad_norm": 0.3657277524471283,
      "learning_rate": 1.531049250535332e-06,
      "loss": 1.1418,
      "step": 425
    },
    {
      "epoch": 2.2764119601328905,
      "grad_norm": 0.3563482165336609,
      "learning_rate": 1.4775160599571737e-06,
      "loss": 1.2337,
      "step": 430
    },
    {
      "epoch": 2.302990033222591,
      "grad_norm": 0.5307823419570923,
      "learning_rate": 1.4239828693790151e-06,
      "loss": 0.6119,
      "step": 435
    },
    {
      "epoch": 2.3295681063122924,
      "grad_norm": 0.36389169096946716,
      "learning_rate": 1.3704496788008568e-06,
      "loss": 0.8084,
      "step": 440
    },
    {
      "epoch": 2.3561461794019936,
      "grad_norm": 0.7799708843231201,
      "learning_rate": 1.3169164882226982e-06,
      "loss": 1.1332,
      "step": 445
    },
    {
      "epoch": 2.3827242524916943,
      "grad_norm": 0.8895277976989746,
      "learning_rate": 1.26338329764454e-06,
      "loss": 1.2915,
      "step": 450
    },
    {
      "epoch": 2.4093023255813955,
      "grad_norm": 0.36828890442848206,
      "learning_rate": 1.2098501070663811e-06,
      "loss": 0.8655,
      "step": 455
    },
    {
      "epoch": 2.435880398671096,
      "grad_norm": 0.23453983664512634,
      "learning_rate": 1.1563169164882228e-06,
      "loss": 0.9536,
      "step": 460
    },
    {
      "epoch": 2.4624584717607974,
      "grad_norm": 0.7493208646774292,
      "learning_rate": 1.1027837259100643e-06,
      "loss": 1.0151,
      "step": 465
    },
    {
      "epoch": 2.4890365448504985,
      "grad_norm": 0.5289668440818787,
      "learning_rate": 1.049250535331906e-06,
      "loss": 0.9903,
      "step": 470
    },
    {
      "epoch": 2.5156146179401992,
      "grad_norm": 0.35549259185791016,
      "learning_rate": 9.957173447537474e-07,
      "loss": 0.8992,
      "step": 475
    },
    {
      "epoch": 2.5421926910299004,
      "grad_norm": 0.3587372899055481,
      "learning_rate": 9.421841541755889e-07,
      "loss": 1.0404,
      "step": 480
    },
    {
      "epoch": 2.568770764119601,
      "grad_norm": 0.37513110041618347,
      "learning_rate": 8.886509635974305e-07,
      "loss": 0.9318,
      "step": 485
    },
    {
      "epoch": 2.5953488372093023,
      "grad_norm": 0.8574331402778625,
      "learning_rate": 8.351177730192721e-07,
      "loss": 1.2517,
      "step": 490
    },
    {
      "epoch": 2.6219269102990035,
      "grad_norm": 0.43273162841796875,
      "learning_rate": 7.815845824411135e-07,
      "loss": 0.437,
      "step": 495
    },
    {
      "epoch": 2.648504983388704,
      "grad_norm": 0.3647739887237549,
      "learning_rate": 7.280513918629551e-07,
      "loss": 0.8412,
      "step": 500
    }
  ],
  "logging_steps": 5,
  "max_steps": 567,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.838789438692393e+18,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
